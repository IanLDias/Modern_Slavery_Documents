{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Need to run !python -m spacy download en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path as pathl\n",
    "import sys, os, re\n",
    "from heapq import nlargest\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append relevant file paths\n",
    "new_path = pathl('.')\n",
    "parent = new_path.resolve().parent\n",
    "sys.path.append(str(parent))\n",
    "from pdf_parser import pipeline\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = parent / 'Data'\n",
    "def get_text(filename):\n",
    "    'Return text from a filename'\n",
    "    pdf_file = data_path / filename\n",
    "    text_dict = pipeline(filepath = str(pdf_file))\n",
    "    text = list(text_dict.values())\n",
    "    text = sum(text, [])\n",
    "    text = [sentence.strip() for sentence in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see if last file path in sys is the root folder\n",
    "current_dir = os.walk(sys.path[-1] + '/Data')\n",
    "all_text = [] #Term frequency\n",
    "for file in next(current_dir)[-1]:\n",
    "    try:\n",
    "        text = get_text(file)\n",
    "        all_text.append(process_text(text))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = parent / 'Data'\n",
    "def get_text(filename):\n",
    "    'Return text from a filename'\n",
    "    pdf_file = data_path / filename\n",
    "    text_dict = pipeline(filepath = str(pdf_file))\n",
    "    text = list(text_dict.values())\n",
    "    text = sum(text, [])\n",
    "    text = [sentence.strip() for sentence in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def lemmatizer(text):\n",
    "    'Lemmatizes text'\n",
    "    doc = nlp.pipe(text)\n",
    "    lemmatized = []\n",
    "    for sentence in doc:\n",
    "        sent = []\n",
    "        for word in sentence:\n",
    "            if str(word) in punctuation:\n",
    "                continue\n",
    "            lemma = word.lemma_.strip() \n",
    "            sent.append(lemma)\n",
    "        \n",
    "        lemmatized.append(' '.join(sent))\n",
    "    return lemmatized\n",
    "\n",
    "def word_frequency(lemmatized_text):\n",
    "    'Needs preprocessed text. Returns counts of words'\n",
    "    bag_of_words = [sentence.split() for sentence in lemmatized_text]\n",
    "    bag_of_words = sum(bag_of_words, [])\n",
    "    return bag_of_words\n",
    "\n",
    "def term_frequency_f(bag_of_words):\n",
    "    'Calculates the term frequency (normalized) from a bag of words'\n",
    "    word_frequency = Counter(bag_of_words)\n",
    "    term_frequency_values = np.array(list(word_frequency.values())) / len(word_frequency)\n",
    "    term_frequency = {}\n",
    "    for word, value in zip(word_frequency.keys(), term_frequency_values):\n",
    "        term_frequency[word] = value\n",
    "    return term_frequency\n",
    "\n",
    "def preprocess(filename):\n",
    "    'Input a filename. Returns a term_frequency count'\n",
    "    text = get_text(filename)\n",
    "    text = text.split('.')\n",
    "    lemmatized = lemmatizer(text)\n",
    "    bag_of_words = word_frequency(lemmatized)\n",
    "    term_frequency = term_frequency_f(bag_of_words)\n",
    "    return term_frequency, bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.walk(sys.path[-1] + '/Data')\n",
    "files = []\n",
    "for file in current_dir:\n",
    "    files.append(file[-1])\n",
    "files = sum(files, [])\n",
    "\n",
    "texts = []\n",
    "lexicon = [] #contains a list of words that exist in all files\n",
    "for file in files:\n",
    "    if file[-3:] == 'pdf':\n",
    "        preprocess_output = preprocess(file)\n",
    "        texts.append(preprocess_output[0])\n",
    "        lexicon.append(preprocess_output[1])\n",
    "lexicon = set(sum(lexicon, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequency = []\n",
    "for doc in texts:\n",
    "    zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "    for key, value in zip(doc.keys(), doc.values()):\n",
    "        zero_vector[key] = value\n",
    "    document_frequency.append(zero_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
